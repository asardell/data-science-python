# Chapitre 8 : Apprentissage Non Supervis√© ‚Äì Clustering

# Introduction √† l‚Äôapprentissage non supervis√©

L‚Äô**apprentissage non supervis√©** consiste √† analyser des donn√©es **sans variable cible** pour en d√©couvrir la structure. Contrairement √† l‚Äôapprentissage supervis√©, on n‚Äôa pas d‚Äô√©tiquette √† pr√©dire.

Les objectifs principaux sont :

- **D√©tection de groupes (clusters)** dans les donn√©es  
- **R√©duction de dimension et exploration**  
- **D√©tection d‚Äôanomalies** ou outliers  

Nous nous concentrons ici sur le **clustering**, pour segmenter les logements ADEME selon leurs caract√©ristiques √©nerg√©tiques ou de consommation.

- [Chapitre 8 : Apprentissage Non Supervis√© ‚Äì Clustering](#chapitre-8--apprentissage-non-supervis√©--clustering)
- [Introduction √† l‚Äôapprentissage non supervis√©](#introduction-√†-lapprentissage-non-supervis√©)
- [K-Means](#k-means)
    - [Principe](#principe)
    - [Configuration et param√®tres](#configuration-et-param√®tres)
    - [Interpr√©tation](#interpr√©tation)
    - [Exemple p√©dagogique](#exemple-p√©dagogique)
    - [Exemple sur le jeu de donn√©es ADEME](#exemple-sur-le-jeu-de-donn√©es-ademe)
- [CAH (Clustering Ascendant Hi√©rarchique)](#cah-clustering-ascendant-hi√©rarchique)
    - [Principe](#principe-1)
    - [Configuration](#configuration)
    - [Interpr√©tation](#interpr√©tation-1)
    - [Exemple p√©dagogique](#exemple-p√©dagogique-1)
    - [Exemple sur le jeu de donn√©es ADEME](#exemple-sur-le-jeu-de-donn√©es-ademe-1)
- [DBSCAN](#dbscan)
    - [Principe](#principe-2)
    - [Param√®tres](#param√®tres)
    - [Interpr√©tation](#interpr√©tation-2)
    - [Exemple p√©dagogique](#exemple-p√©dagogique-2)
    - [Exemple sur le jeu de donn√©es ADEME](#exemple-sur-le-jeu-de-donn√©es-ademe-2)
  - [4. √âvaluation des clusters](#4-√©valuation-des-clusters)
    - [Calcul du coefficient de silhouette](#calcul-du-coefficient-de-silhouette)
  - [5. Synth√®se des m√©thodes](#5-synth√®se-des-m√©thodes)
- [Exercice : Clustering combin√© sur les logements DPE](#exercice--clustering-combin√©-sur-les-logements-dpe)
  - [Objectif](#objectif)
  - [Donn√©es √† utiliser](#donn√©es-√†-utiliser)
  - [√âtapes de l‚Äôexercice](#√©tapes-de-lexercice)
    - [Pr√©paration des donn√©es](#pr√©paration-des-donn√©es)
    - [Transformation des variables qualitatives](#transformation-des-variables-qualitatives)
    - [Clustering](#clustering)
    - [Analyse des clusters](#analyse-des-clusters)


# K-Means

### Principe

- Partitionne les donn√©es en **K clusters** pr√©d√©finis.  
- Chaque point appartient au cluster dont le **centre (centro√Øde)** est le plus proche.  
- Objectif : **minimiser la variance intra-cluster** (somme des distances au carr√© par rapport au centro√Øde).

### Configuration et param√®tres

- `n_clusters` : nombre de clusters K  
- `init` : m√©thode d‚Äôinitialisation des centro√Ødes (`k-means++` recommand√©)  
- `n_init` : nombre de r√©initialisations pour choisir la meilleure solution  
- `max_iter` : nombre maximum d‚Äôit√©rations  
- `random_state` : pour reproductibilit√©  

### Interpr√©tation

- **Centro√Ødes** : valeur moyenne des variables dans chaque cluster  
- **Labels** : cluster assign√© √† chaque observation  
- **Inertia** : somme des distances au carr√© des points √† leur centro√Øde  
- **Silhouette score** : mesure de coh√©rence des clusters (proche de 1 = bon cluster, proche de 0 = chevauchement, <0 = erreur)

### Exemple p√©dagogique

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Cr√©ation d'un dataset factice
X, _ = make_blobs(n_samples=200, centers=3, n_features=2, random_state=42)

# KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# Visualisation
plt.scatter(X[:,0], X[:,1], c=labels, cmap='viridis')
plt.scatter(centroids[:,0], centroids[:,1], color='red', marker='X', s=200)
plt.title("K-Means - exemple p√©dagogique")
plt.show()
```

### Exemple sur le jeu de donn√©es ADEME

```python
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import pandas as pd

# Exemple : colonnes num√©riques
cols = ['surface_habitable_logement', 'besoin_chauffage', 'conso_chauffage_ef']
df_ade = df[cols].dropna()  # nettoyage minimal
X = StandardScaler().fit_transform(df_ade)

# KMeans
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X)
df_ade['cluster'] = labels

# √âvaluation
score = silhouette_score(X, labels)
print("Silhouette score :", score)

# Analyse des centro√Ødes
centroids = pd.DataFrame(kmeans.cluster_centers_, columns=cols)
print(centroids)
```

> Interpr√©tation : chaque cluster correspond √† un groupe de logements avec caract√©ristiques similaires. Le centro√Øde montre la **valeur moyenne des variables dans le cluster**, utile pour d√©crire le profil type.


# CAH (Clustering Ascendant Hi√©rarchique)

### Principe

- Construire une **hi√©rarchie de clusters** sans pr√©-d√©finir le nombre de clusters.  
- On commence avec **chaque observation comme cluster**, puis on **fusionne progressivement** les clusters les plus proches.  
- R√©sultat : **dendrogramme** montrant la fusion des clusters.

### Configuration

- `affinity` : m√©trique de distance (`euclidean` par d√©faut)  
- `linkage` : m√©thode de fusion (`ward`, `complete`, `average`)  
  - `ward` ‚Üí minimise variance intra-cluster  
  - `complete` ‚Üí distance max entre points de clusters  
  - `average` ‚Üí distance moyenne  
- Pas besoin de sp√©cifier K initialement, on coupe le dendrogramme √† la hauteur souhait√©e.

### Interpr√©tation

- **Dendrogramme** : visualiser comment les observations se regroupent  
- **Coupe du dendrogramme** : choix du nombre optimal de clusters  
- **Silhouette score** possible pour √©valuer la qualit√© des clusters

### Exemple p√©dagogique

```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Dataset factice
X, _ = make_blobs(n_samples=50, centers=3, n_features=2, random_state=42)

# CAH
Z = linkage(X, method='ward')
plt.figure(figsize=(8,5))
dendrogram(Z)
plt.title("Dendrogramme - CAH")
plt.show()

# Couper √† 3 clusters
labels = fcluster(Z, 3, criterion='maxclust')
```

### Exemple sur le jeu de donn√©es ADEME

```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

X = StandardScaler().fit_transform(df_ade[cols])
Z = linkage(X, method='ward')

# Dendrogramme
plt.figure(figsize=(10,6))
dendrogram(Z, truncate_mode='level', p=5)
plt.title("Dendrogramme CAH - ADEME")
plt.show()

# Attribution des clusters
df_ade['cluster_cah'] = fcluster(Z, 4, criterion='maxclust')
```


# DBSCAN

### Principe

- **Density-Based Spatial Clustering of Applications with Noise**  
- Forme des clusters bas√©s sur la **densit√© locale des points**.  
- Avantages : d√©tection d‚Äô**outliers** automatiquement, pas besoin de fixer le nombre de clusters.

### Param√®tres

- `eps` : rayon de voisinage pour former un cluster  
- `min_samples` : nombre minimum de points dans ce rayon pour cr√©er un cluster  
- Points isol√©s ‚Üí **label = -1** (outliers)

### Interpr√©tation

- Points avec **label ‚â•0** : appartiennent √† un cluster  
- Points avec **label = -1** : consid√©r√©s comme anomalies ou bruit  
- Visualisation possible pour v√©rifier densit√© et structure

### Exemple p√©dagogique

```python
from sklearn.cluster import DBSCAN

X, _ = make_blobs(n_samples=100, centers=3, n_features=2, random_state=42)

dbscan = DBSCAN(eps=1.0, min_samples=5)
labels = dbscan.fit_predict(X)

plt.scatter(X[:,0], X[:,1], c=labels, cmap='plasma')
plt.title("DBSCAN - exemple p√©dagogique")
plt.show()
```

### Exemple sur le jeu de donn√©es ADEME

```python
dbscan = DBSCAN(eps=1.5, min_samples=10)
labels = dbscan.fit_predict(X)
df_ade['cluster_dbscan'] = labels

# Analyse des outliers
outliers = df_ade[df_ade['cluster_dbscan']==-1]
print("Nombre d'outliers :", len(outliers))
```


## 4. √âvaluation des clusters

M√™me sans variable cible, on peut √©valuer la qualit√© des clusters :

- **Silhouette score** : mesure la coh√©rence intra-cluster et s√©paration inter-cluster  
  - Valeur proche de 1 ‚Üí clusters bien s√©par√©s  
  - Valeur proche de 0 ‚Üí chevauchement  
  - Valeur n√©gative ‚Üí mauvais clustering

- **Inertia (pour K-Means)** : somme des distances au carr√© aux centro√Ødes  

- **Observation des centro√Ødes / moyenne des variables** pour interpr√©ter chaque cluster

### Calcul du coefficient de silhouette

```python
from sklearn.metrics import silhouette_score

# KMeans
silhouette_kmeans = silhouette_score(X, df_ade['cluster'])
print("Silhouette KMeans :", silhouette_kmeans)

# CAH
silhouette_cah = silhouette_score(X, df_ade['cluster_cah'])
print("Silhouette CAH :", silhouette_cah)

# DBSCAN (ignorer outliers pour le score)
mask = df_ade['cluster_dbscan'] != -1
silhouette_dbscan = silhouette_score(X[mask], df_ade['cluster_dbscan'][mask])
print("Silhouette DBSCAN :", silhouette_dbscan)
```


## 5. Synth√®se des m√©thodes

| M√©thode     | Nombre clusters | D√©tection outliers | Type de clusters | Interpr√©tation |
|------------|----------------|------------------|-----------------|---------------|
| K-Means    | fixe           | Non              | sph√©rique        | centro√Ødes, inertia |
| CAH        | flexible       | Non              | hi√©rarchique     | dendrogramme, silhouette |
| DBSCAN     | automatique    | Oui              | densit√©          | labels, points isol√©s |


üí° **Conseils pratiques** :

- Toujours **standardiser les variables** avant clustering  
- Explorer **plusieurs m√©thodes et param√®tres** pour comparer  
- Visualiser les clusters en 2D ou 3D pour valider  
- K-Means et CAH ‚Üí mieux pour clusters globaux  
- DBSCAN ‚Üí mieux pour d√©tecter des anomalies ou clusters irr√©guliers


# Exercice : Clustering combin√© sur les logements DPE

## Objectif
- Appliquer la **MCA** pour transformer les variables qualitatives en variables num√©riques continues.  
- Combiner les composantes MCA avec des variables num√©riques standardis√©es.  
- Appliquer un **algorithme de clustering** sur les donn√©es mixtes.  
- Interpr√©ter les clusters obtenus pour identifier des groupes de logements similaires.


## Donn√©es √† utiliser
- Variables qualitatives :  
  - `etiquette_dpe`  
  - `type_batiment`  
- Variables num√©riques :  
  - `surface_habitable_logement`  
  - `besoin_chauffage`  
  - `conso_chauffage_ef`  
  - `conso_ecs_ef`  


## √âtapes de l‚Äôexercice

### Pr√©paration des donn√©es
1. S√©lectionner les colonnes qualitatives et num√©riques.  
2. G√©rer les valeurs manquantes si n√©cessaire (imputation ou suppression).  
3. Standardiser les variables num√©riques pour qu‚Äôelles soient comparables.

### Transformation des variables qualitatives
1. Appliquer la **MCA** sur les variables qualitatives.  
2. Extraire un nombre choisi de composantes principales (ex. 2 √† 5).  
3. Ajouter ces composantes au jeu de donn√©es standardis√©.

### Clustering
1. Choisir un algorithme de clustering : **K-Means**, **CAH**, ou **DBSCAN**.  
2. Appliquer le clustering sur le jeu de donn√©es mixte (composantes MCA + variables num√©riques).  
3. Tester diff√©rents param√®tres pour observer leur impact (ex. nombre de clusters k pour K-Means).

### Analyse des clusters
1. Visualiser les clusters sur un scatter plot en utilisant les 2 premi√®res composantes MCA ou les 2 variables num√©riques les plus significatives.  
2. Comparer la distribution des clusters par rapport aux variables qualitatives (`etiquette_dpe`, `type_batiment`).  
3. Discuter des insights obtenus : quels types de logements se regroupent ensemble ? Y a-t-il des comportements √©nerg√©tiques similaires ?
